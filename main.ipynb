{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain for LLM(Palm2) Application Development\n",
    "## https://github.com/hwchase17/langchain/pull/3575"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "# configure palm\n",
    "import google.generativeai as palm\n",
    "api_key = 'AIzaSyBXBk2SKMiiSYJI07uH8C-Yj3PjmMUnki0' # put your API key here\n",
    "palm.configure(api_key=api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model(name='models/text-bison-001', base_model_id='', version='001', display_name='Text Bison', description='Model targeted for text generation.', input_token_limit=8196, output_token_limit=1024, supported_generation_methods=['generateText'], temperature=0.7, top_p=0.95, top_k=40)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choose a model there are 1 model available\n",
    "models = [m for m in palm.list_models() if 'generateText' in m.supported_generation_methods]\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 1 models available\n"
     ]
    }
   ],
   "source": [
    "print('there are {} models available'.format(len(models)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/text-bison-001\n"
     ]
    }
   ],
   "source": [
    "model = models[0].name\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The sky is not green. It is blue.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate text\n",
    "prompt = 'why is the sky green?'\n",
    "text = palm.generate_text(\n",
    "    prompt=prompt,\n",
    "    model=model,\n",
    "    temperature=0.1,\n",
    "    max_output_tokens=64,\n",
    "    top_p=0.9,\n",
    "    top_k=40,\n",
    "    stop_sequences=['\\n']\n",
    ")\n",
    "text.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatResponse(model='models/chat-bison-001', context='', examples=[], messages=[{'author': '0', 'content': 'Hello'}, {'author': '1', 'content': 'Hello! How can I help you today?'}], temperature=0.1, candidate_count=None, candidates=[{'author': '1', 'content': 'Hello! How can I help you today?'}], filters=[], top_p=None, top_k=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new conversation\n",
    "response = palm.chat(messages='Hello',\n",
    "                     temperature=0.1,)\n",
    "\n",
    "# Last contains the model's response:\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I help you today?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.last"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import GooglePalmEmbeddings\n",
    "from langchain.llms import GooglePalm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = GooglePalm()\n",
    "llm.temperature = 0.1\n",
    "\n",
    "prompts = [\"The opposite of hot is\",'The opposite of cold is'] # according to the class prmpts must be in list\n",
    "llm_result = llm._generate(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cold'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_result.generations[0][0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hot'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_result.generations[1][0].text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why are the results stored in a **multidimensional array?**\n",
    "\n",
    "Imagine you are working with a bard. \n",
    "\n",
    "- You provide **1** input to the bard. \n",
    "- The bard processes it and returns **3** outputs. \n",
    "\n",
    "Similarly, in this case, we can obtain a maximum of **8** outputs. \n",
    "\n",
    "- The first dimension represents each prompt in the prompts list.\n",
    "- The second dimension represents the answers for each input. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredURLLoader  #load urls into docoument-loader\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.indexes import VectorstoreIndexCreator #vectorize db index with chromadb\n",
    "from langchain.text_splitter import CharacterTextSplitter #text splitter\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "from langchain.document_loaders import UnstructuredPDFLoader  #load pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = ['https://www.linkedin.com/pulse/transformers-without-pain-ibrahim-sobh-phd/',]\n",
    "loader = [UnstructuredURLLoader(urls=urls)]\n",
    "index = VectorstoreIndexCreator(\n",
    "        embedding=GooglePalmEmbeddings(),\n",
    "        text_splitter=CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)).from_loaders(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RetrievalQA.from_chain_type(llm=llm, \n",
    "                            chain_type=\"stuff\", \n",
    "                            retriever=index.vectorstore.as_retriever(), \n",
    "                            input_key=\"question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Machine Translation (MT) is the task of translating a sentence x from one language (the source language) to a sentence y in another language (the target language).'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = chain.run('What is machine translation?')\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Books/GANs and diff personal notes.pdf']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_books_paths = [os.path.join('Books', p) for p in os.listdir('Books')]\n",
    "all_books_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GANs are a type of machine learning model that learns to generate new data that is similar to a set of existing data. GANs are trained using a technique called \"adversarial learning,\" in which two neural networks compete with each other.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_folder_path = 'Books'\n",
    "pdf_loaders = [UnstructuredPDFLoader(os.path.join(pdf_folder_path, fn)) for fn in os.listdir(pdf_folder_path)]\n",
    "pdf_index = VectorstoreIndexCreator(\n",
    "        embedding=GooglePalmEmbeddings(),\n",
    "        text_splitter=CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)).from_loaders(pdf_loaders)\n",
    "\n",
    "pdf_chain = RetrievalQA.from_chain_type(llm=llm,\n",
    "                            chain_type=\"stuff\",\n",
    "                            retriever=pdf_index.vectorstore.as_retriever(),\n",
    "                            input_key=\"question\")\n",
    "\n",
    "pdf_answer = pdf_chain.run('What are GANs?')\n",
    "pdf_answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an artificial intelligence assistant work in academia. You are asked to answer questions. The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
      "\n",
      "Who are you?\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "I am an artificial intelligence assistant that works in academia. I am designed to help researchers and students by answering their questions, providing them with information, and suggesting resources. I am able to access and process information from a variety of sources, including the internet, academic databases, and scholarly journals. I am also able to generate text, translate languages, and write different kinds of creative content.\n",
      "\n",
      "I am still under development, but I am learning new things every day. I am excited to continue to help researchers and students in academia, and I look forward to the day when I can be an even more valuable resource.\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "You are an artificial intelligence assistant work in academia. You are asked to answer questions. The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
    "\n",
    "{question}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm, verbose=True)\n",
    "\n",
    "print(llm_chain.run('Who are you?'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
